import h5py
import numpy as np

import glob, os, re

from typing import cast, Iterator


# Models found within the GWTC datasets.
MODELS = [
    'IMRPhenomXPHM_comoving', 'IMRPhenomXPHM', 'IMRPhenomPv3HM',
    'IMRPhenomPv2', 'NRSur7dq4', 'SEOBNRv4PHM', 'PrecessingSpinIMRHM',
    'SEOBNRv4P',
]

# The NumPy data type of the simulation models.
MODEL_DTYPE = [
    ('a_final', np.float64),
    ('mass_final[Msun]', np.float64),
    ('DL[Mpc]', np.float64),
    ('inclination[rad]', np.float64),
    ('OpeningAngleU10-40deg[rad]', np.float64),
    ('FQQ', np.float64),
    ('FNUNU', np.float64),
    ('FBZ', np.float64),
    ('FGW', np.float64),
]


class Event(np.ndarray):
    DTYPE = [
        ('inclination', np.float64),
        ('opening_angle', np.float64),
        ('flux_QQ', np.float64),
        ('flux_NU', np.float64),
        ('flux_BZ', np.float64),
        ('flux_GW', np.float64),
        ('right_ascension', np.float64),
        ('declination', np.float64),
        ('upper_limit', np.float64),
    ]

    def __new__(cls, simulations: int):
        shape = (simulations)
        return super().__new__(cls, shape, dtype = Event.DTYPE)


def wildcard(directory: str, extension: str) -> Iterator[str]:
    '''Searches for files ending with the given `extension` within
    `directory` and yields the file path of each matching file.

    Args:
        directory (str): The directory to search within for files.
        extension (str): The file extension to limit the search to.

    Yields:
        str: The file path of each matching file.
    '''

    files = glob.glob(os.path.join(directory, f'*.{extension}'))
    for path in files:
        yield path


def extract_identifier(path: str) -> str:
    '''Extracts an identifier (name) of a gravitational wave event
    from a file path if it is present.

    Args:
        path (str): The file path to extract an identifier from.

    Returns:
        str: The identifier (name) of the gravitational wave event,
            or 'Unknown' if no identifier is present.
    '''

    pattern = re.compile(r'GW\d+_?(?=\d)\d*')
    m = pattern.search(path)
    if m:
        return m.group(0)
    return 'Unknown'


def extract_model(path: str) -> str:
    '''Extract the model used for a set of simulations from a file
    path. If the model is not found within GWTC datasets, 'Unknown'
    is returned instead.

    Args:
        path (str): The file path to extract a model from.

    Returns:
        str: The extracted model, or 'Unknown' if the model is
            not found within GWTC datasets.
    '''

    for model in MODELS:
        if model in path:
            return model
    return 'Unknown'


def find_models(directory: str) -> dict[str, str]:
    '''Searches for models within a given `directory` and returns a
    dictionary containing the identifier, model, and file path of each.

    Args:
        directory (str): The directory to search for models within.

    Returns:
        dict[str, str]: A dictionary containing the identifier and
            model of each event (as the keys) and the file path to a
            text file containing the model data (as the values).
    '''

    models = {}

    paths = sorted(wildcard(directory, 'txt'))
    for path in paths:
        identifier = extract_identifier(path)
        model = extract_model(path)
        key = f'{identifier}_{model}'
        models[key] = path

    return models


def trim_models(models: dict[str, str]) -> dict[str, str]:
    '''Trims duplicate models from the dictionary generated by
    `find_models` so that each event has a single model associated
    with it.

    Args:
        models (dict[str, str]): A dictionary containing the identifier
            and model of each event (as the keys) and the file path to
            a text file containing the model data (as the values).

    Returns:
        dict[str, str]: A dictionary containing the identifier and
            model of each event (as the keys) and the file path to a
            text file containing the model data (as the values), with
            duplicate models for each event removed.
    '''

    keeps = []

    for key in sorted(models.keys()):
        identifier = extract_identifier(key)
        model = extract_model(key)

        # If the model is not found within GWTC datasets, skip it.
        if model == 'Unknown':
            continue

        skip = False
        for keep in keeps:
            # If an event with the same identifier has already been seen,
            # skip it, as we don't want any duplicates. IMRPhenom* models
            # will be preferred because of the alphabetisation by sorting.
            if identifier == extract_identifier(keep):
                skip = True
                break
        if skip:
            continue

        keeps.append(key)

    # Return a copy of the models dictionary with only the keys in `keeps`.
    return { key: models[key] for key in keeps }


def load_models(models: dict[str, str]) -> dict[str, Event]:
    '''Loads the models within the `models` dictionary generated by
    `find_models` into NumPy arrays.

    Args:
        models (dict[str, str]): A dictionary containing the identifier
            and model of each event (as the keys) and the file path to
            a text file containing the model data (as the values).

    Returns:
        dict[str, Event]: A dictionary containing the identifier and
            model of each event (as the keys) and a NumPy array with
            the data type `Event.DTYPE` with the data loaded from the
            model files (as the values).
    '''

    events = {}

    for key, path in models.items():
        data = np.loadtxt(path, dtype = MODEL_DTYPE)

        event = Event(data.size)
        event['inclination'] = data['inclination[rad]']
        event['opening_angle'] = data['OpeningAngleU10-40deg[rad]']
        event['flux_QQ'] = data['FQQ']
        event['flux_NU'] = data['FNUNU']
        event['flux_BZ'] = data['FBZ']
        event['flux_GW'] = data['FGW']

        # Initialise the rest of the array to NaN instead of garbage.
        event['right_ascension'] = np.nan
        event['declination'] = np.nan
        event['upper_limit'] = np.nan

        events[key] = event

        identifier = extract_identifier(key)
        model = extract_model(key)
        print(f'Read {identifier} ({model}) from {os.path.basename(path)}.')

    return events


def find_places(directory: str) -> dict[str, str]:
    '''Searches for GWTC datasets within a given `directory` and
    returns a dictionary containing the identifier of each event and
    the file path of its associated dataset.

    Args:
        directory (str): The directory to search for datasets within.

    Returns:
        dict[str, str]: A dictionary containing the identifier of each
            event (as the keys) and the file path to its associated
            GWTC-2, GWTC-2.1 or GWTC-3 dataset (as the values).
    '''

    places = {}

    paths = sorted(wildcard(directory, 'h5'))
    for path in paths:
        identifier = extract_identifier(path)
        places[identifier] = path

    return places


def main() -> None:
    # Find, trim and finally load the simulation models.
    directory = os.path.join('data', 'modelling')
    models = find_models(directory)
    models = trim_models(models)
    events = load_models(models)

    print(f'Read {len(events)} total events.')
    print()

    directory = os.path.join('data', 'GWTC')
    places = find_places(directory)

    count = 0
    for key, event in events.items():
        identifier = extract_identifier(key)
        path = None

        # If there's an exact match for the identifier...
        if identifier in places:
            path = places[identifier]
        else:
            # Otherwise, search for a similar identifier.
            for i, p in places.items():
                if identifier in i:
                    path = p
                    break

        if path is None:
            print(f'No GWTC dataset was found for {identifier}!')
            continue

        with h5py.File(path) as f:
            model = extract_model(key)
            model_key = model

            # Determine if the model exists within the GWTC dataset and in
            # which format, as some datasets expect only the name of the model
            # and others use the format C01:{model}.
            if model_key not in f.keys():
                model_key = f'C01:{model}'
                if model_key not in f.keys():
                    print(f'{model} is not present in the GWTC dataset for ' \
                      f'{identifier}!')
                    continue

            data = f[model_key]['posterior_samples'] # type: ignore

            # Check if there is a shape mismatch.
            if event.shape != data['ra'].shape: # type: ignore
                print(f'Ignoring {identifier} ({model}) from {path} due to ' \
                       'shape mismatch.')
                continue

            event['right_ascension'] = cast(np.ndarray, data['ra']) # type: ignore
            event['declination'] = cast(np.ndarray, data['dec']) # type: ignore

        events[key] = event

        print(f'Read coordinates of {identifier} ({model}) from ' \
              f'{os.path.basename(path)}.')
        count += 1

    print(f'Read {count} locations.')


if __name__ == '__main__':
    main()
